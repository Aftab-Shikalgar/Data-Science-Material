{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788db2ab-8c15-4381-b6c2-fb456f9f75e1",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\">Ensemble Techniques</h1>\n",
    "\n",
    "<h3 style=\"color:orange;line-height:1.2\">An ensemble technique is a machine learning technique that combines multiple models to improve the accuracy of predictions. The goal is to create a more robust model by capturing the strengths of each model while reducing their weaknesses</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c8d3aa7-2765-47bd-89c5-1a99531cd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 3 method to do so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce520b-9f71-4f45-840e-1c47646ca704",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;padding:20px\">\n",
    "\n",
    "<h3>Bagging - </h3>\n",
    "\n",
    "- Often considers __homogenous weak learners(models)__   i.e. same models\n",
    "- Learns from them __independently from in other in parallel__\n",
    "- And __combines them following some kind of deterministic averaging process__\n",
    "\n",
    "____\n",
    "\n",
    "<h3>Boosting -</h3>\n",
    "\n",
    "- Often considers __homogenous weak learners(models) too__\n",
    "- Learns them __sequentially in very adaptive way(a base model depends on the previous one)__\n",
    "- And __combines them following a deterministic strategy__\n",
    "\n",
    "____\n",
    "\n",
    "<h3>Stacking - </h3>\n",
    "\n",
    "- Often considers __heterogenous weak learners__ i.e. different models\n",
    "- Learns them __in parallel independently__\n",
    "- And combines them by __training a meta-model to output a prediction based on the different weak model's predictions__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c499bc41-f1c7-4d18-a6b6-bc2fde6d9b24",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D1.png\" alt=\"image\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a972f-1537-4f4c-b000-e0238238caa3",
   "metadata": {},
   "source": [
    "- First, __input training data__ is used to build a number of models\n",
    "- The __allocation function__ dictates how much of the training data each model recieves\n",
    "  - Do they each receive the full training dataset or merely a sample?\n",
    "  - Do they each receive every feature or a subset?\n",
    "- After the models are constructed, they can be used to genarate a set of predictions, which must be managed in some way\n",
    "- The __combination function__ governs how disagreements among the predictions are reconciled\n",
    "  - e.g. The ensemble might use a majority vote to determine the final prediction, or it could use a more complex strategy such as\n",
    "    weighting each model's votes based on its prior performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1eeb1-3326-4fde-817d-72063e577d75",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D2.png\" alt=\"image\" width=\"800px\" border=\"2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee6dea-2b46-4bd0-be5e-4a8401e5a246",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">1. Bagging -</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e55f12-8b31-4b45-94c8-1b5db7a81120",
   "metadata": {},
   "source": [
    "- One of the first to gain widespread acceptance used as a technique called __Boostrap Aggregating__ or __Bagging__\n",
    "- Bagging generates a number of training datasets by __bootstrap sampling(sampling with replacement)__ the original training data\n",
    "- These datasets are then used to generate a set of models using a __single learning algorithm__\n",
    "- The models' predictions are __combined using voting(for classification)__ or __averaging(for regression)__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d764c709-b09b-407f-8b92-89c24c24be61",
   "metadata": {},
   "source": [
    "Although bagging is relatively simple ensemble, it can perform quite well as long as it is used with relatively UNSTABLE learners\n",
    "i.e. those models that tend to change substantially when the input data changes only slightly\n",
    "\n",
    "Unstable models are essential in order to ensure the ensemble's diversity\n",
    "\n",
    "For this reason, bagging is often used with decision trees, which have the tedency to vary dramatically given minor changes in input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bdcb4-ed31-44e3-8892-119028d5c6a5",
   "metadata": {},
   "source": [
    "<h2 style=\"color:limegreen\">Bootstrap Method - </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167602f-6563-4d79-8938-c9114316e024",
   "metadata": {},
   "source": [
    "- It is a powerful statistical method for estimating a quantity from a data sample\n",
    "- This is easiest way to understand if the quantity is a descriptive statistic such as mean or std deviation\n",
    "\n",
    "____\n",
    "\n",
    "__Let's assume we have sample of 100 values(x) and we'd like to get an estimate of the mean of the sample__\n",
    "\n",
    "- We can directly calculate the mean as __mean = sum(x)/100__\n",
    "- We know our sample is small and that our mean has error in it. We can improve the estimate using following __Bootstrap Procedure:__\n",
    "  - Create many(say 1000) random sub-samples of our dataset with replacement(values can repeat)\n",
    "  -  Calculate mean of each sub-sample\n",
    "  -  Calculate average of all means of sub-samples\n",
    "  -  Use this as our sample mean\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "__Let's assume now, we have a sample dataset of 1000 instances(x) and we are using the C5.0 algorithm__\n",
    "\n",
    "\n",
    "Bagging would work as follows:\n",
    "\n",
    "- Create many(say 100) random sub-samples with replacement\n",
    "- Train a C5.0 on each sub-sample\n",
    "- Given a new dataset, calculate the avearage prediction from each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e2271-ed83-44bf-8542-53e6dc83b3ae",
   "metadata": {},
   "source": [
    "## ...continued\n",
    "\n",
    "- When bagging with decision trees, __we are less concerned about individual trees overfitting the training data__\n",
    "- For this reason, the __individual trees are grown deep without pruning__\n",
    "- These trees will have both __high variance and low bias__\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50a5b456-fe06-47bc-a26b-b76eac430721",
   "metadata": {},
   "source": [
    "The only parameters when bagging decision trees is the number of samples and hence the number of trees to include\n",
    "\n",
    "This can be chosen by increasing number of trees on run after run until accuracy begind to stop showing improvment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367ba28-079b-4452-96e8-bf4c018e04d6",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">1.1. Random Forest -</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2439f1a-1629-416f-aec0-233f30af6c25",
   "metadata": {},
   "source": [
    "<h3>The Random Forest is a model made up of many decision trees.</h3>\n",
    "\n",
    "<div style=\"background-color:beige;padding:20px;border:1px solid black\">\n",
    "    \n",
    "#### Rather than simply averaging predictions of trees, this model uses 2 key concepts that gives it the name random :\n",
    "\n",
    "- Random sampling of training data points when building trees\n",
    "- Random subsets of features considered when splitting nodes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45271569-24ed-4048-a077-38110effc316",
   "metadata": {},
   "source": [
    "- When training, each tree in random forest learns from a random sample of data points\n",
    "- The samples are drawn __with replacement__, known as __booststraping__, which means that some samples will be used multiple times in single tree\n",
    "\n",
    "- The other main concept in random forest is that only a __subset of all the features are considered for splitting each node__ in each decision tree\n",
    "- Generally this is set to __sqrt(n_features) for classifiaction__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f5d8a-6c21-42f6-a563-635c6627830e",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D4.png\" alt=\"image\" width=\"800px\" border=\"2px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4ee9b267-6d64-4540-a55b-3c86ada31995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the way of splitting node is using Gini Impurity or Gini Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e93097cb-e750-4adb-a383-c53098704f34",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D5.png\" alt=\"image\" width=\"500px\" border=\"2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238343fc-3797-4a37-abf9-e68370e7c002",
   "metadata": {},
   "source": [
    "<div style=\"border:3px dotted red;padding:30px\">\n",
    "\n",
    "<h2 style=\"color:limegreen\"> Random Forest Pseudocode - </h2>\n",
    "\n",
    "- Randomly select __\"k\"__ features from total __\"m\"__ features, where __k << m__\n",
    "- Among the __\"k\"__ features, calculate the node __\"d\"__ using the __best split point__\n",
    "- Split the node into __Daughter Nodes__ using the __best split__\n",
    "- Repeat __1 to 3__ steps until __\"i\"__ number of nodes has been reached\n",
    "- Build forest by repeating steps __1 to 4__ for __\"n\"__ number of times to create __\"n\"__ number of trees\n",
    "\n",
    "__To get in deatail explanation check out this PDF__: [Random Forest Steps](Files\\\\Random_Forest_Pseudocode_with_Examples_Fixed.pdf)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef6134-0643-4fb1-ae00-8de91fd93378",
   "metadata": {},
   "source": [
    "<h3 style=\"color:slateblue\">Advantages of Random Forest Algorihtm - </h3>\n",
    "\n",
    "- The same __Random Forest Algorithm__ or random forest classifier can be used for both __classification and regression__ task\n",
    "- Random Forest classifier will __handle the missing values__, still we do it while EDA\n",
    "- The __overfitting problem will never come__ when we use the random forest algorithm in any classification problem\n",
    "- The random forest algorithm __can be used for Feature Engineering__\n",
    "  - which means indentifying the most imp features out of the available features from the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef37e37-65bb-437c-adc0-9de292b8949b",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">2. Boosting -</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b03416-2691-48c2-b768-a4ee5a866180",
   "metadata": {},
   "source": [
    "<h4>Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers</h4>\n",
    "\n",
    "\n",
    "- This is done by building a model from the training data, then creating a second model that __attempts to correct the errors from the <br>\n",
    "    first model__.\n",
    "- Models are added until the training set is predicted perfectly, or a __maximum number of models are added__\n",
    "\n",
    "- __AdaBoost__ was the first successful boosting algorithm developed for binary classification and later on extended to multiclass problems\n",
    "\n",
    "- It can be used to boost performance of any ML algorithm. It is best used with __weak learners__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "89f102c4-5a7c-4309-9bbf-be91afb2239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first try to understand roughly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8e12a-526d-423a-8859-a84fc390e39e",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D6.png\" alt=\"image\" width=\"800px\" border=\"2px\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b510c463-b8d2-40c6-9c5c-c6c503072f4d",
   "metadata": {},
   "source": [
    "1 - As we can see, suppose there are 100 records and I want to perfrom boosting. We selected model counts as 200 for boosting.\n",
    "\n",
    "2 - Let's suppose M1 has 20% error or misclassified data. And let's suppose each model require 100 datapoints\n",
    "\n",
    "3 - So for the model M2, we will first pass that 20% missclassfied data for training and other 80 datapoints are chosen randomly\n",
    "    from 100 available datapoints. Datapoints can be repeated for a model\n",
    "\n",
    "4 - The question is how to pass missclassified data to next model?\n",
    "\n",
    "5 - -> The technique assign weight to each datapoint as 1/n i.e. 1/100 at first\n",
    "    -> Then as per the diagram, after builing each model, stage is calculated for each datapoint\n",
    "    -> After that weight of each datapoint is updated  with (1/n)*(e^(stage*Percent_error))   # refer the bottom half part\n",
    "    -> Look at the part below in the diagram, it can be seen that weights are updated of only datapoints which have error in classifying\n",
    "    -> The datapoints with updated weights are considered first for next model on priority\n",
    "\n",
    "6 - The output of M200 will be considered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b3841-7490-4fdd-98f6-93687b1fd632",
   "metadata": {},
   "source": [
    "<h2 style=\"color:limegreen\"> Boosting Algorithm - </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68210d2b-f9f3-4c91-b447-7618ea313d2a",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D7.png\" alt=\"image\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6ae14-8c43-4048-aece-0fab539e065d",
   "metadata": {},
   "source": [
    "- __Samples__ difficult to classify receives increasingly larger weights until the algorithm identifies an algorithm that correctly <br>\n",
    "    classifies these examples\n",
    "\n",
    "- At each iteration, a __stage weight [ln(1-err/err)]__ is computed based on the error rate at that itertaion\n",
    "\n",
    "- At the initial stage it assigns equal weight __1/N__ to all observations\n",
    "\n",
    "- The overall sequence of weighted classifiers is then __combined__ into an ensemble and has a __strong potential__ to classify better <br>\n",
    "    than any of the individual classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863ed92-4185-4f62-95b9-00c34cfd9661",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D8.png\" alt=\"image\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1e16d-72c2-45ce-b9c3-2a789c2955d0",
   "metadata": {},
   "source": [
    "<h2 style=\"color:limegreen\"> Learning an AdaBoost Model from Data - </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654667ff-52c8-4d3d-b27c-f2b8c3977769",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D9.png\" alt=\"image\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b8583-27d6-4025-be5b-22b06ab31a5f",
   "metadata": {},
   "source": [
    "- Each sample have the same starting weight __1/N__ initially\n",
    "  \n",
    "- Fit a weak classifier using the weighted samples and __compute the k'th model's misclassification error(errk)__\n",
    "- Compute the kth stage value as __ln((1-errk)/errk)__\n",
    "- __Update the sample weights giving more weight to incorrectly predicted samples__ and less weight to correctly predicted samples\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4faa4c95-7ca5-4c22-a09a-ec50d892e69d",
   "metadata": {},
   "source": [
    "Where error is the mis-classification rate, correct are the number of training instance predicted correctly by the model\n",
    "and N is the total number of training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97b839-25aa-4d0f-88ee-6e0668799a2c",
   "metadata": {},
   "source": [
    "- e.g. If the model predicted 78 of 100 training instances correctly the error or mis-classification rate would be __78-100/100 = 0.22__\n",
    "\n",
    "  <img src=\"images\\\\Ensemble_Techniques_D10.png\" alt=\"image\" width=\"200px\" style=\"margin-left:30%\">\n",
    "\n",
    "- The above formula is modified to use the weightage of the training instances:\n",
    "\n",
    "  <img src=\"images\\\\Ensemble_Techniques_D11.png\" alt=\"image\" width=\"200px\" style=\"margin-left:30%\">\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba34a29-6c1f-4879-b57c-ab93f98a164a",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D12.png\" alt=\"image\" width=\"700px\" border=\"2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbc9e0-cdd4-491e-8694-466da286b355",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">3. Stacking -</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02418e9-db8c-469b-84bc-e733ab9ea8e4",
   "metadata": {},
   "source": [
    "<h4 style=\"line-height:1.5\">Building multiple models(typically of differing types) and supervisor model that learns how to best combine the predictions <br>\n",
    "    of the primary models\n",
    "</h4>\n",
    "\n",
    "- Suppose in __C1 we used Decision Tree(C5.0)__, __C2 -> CART__ and __C3 -> Random Forest__\n",
    "- Then from theses classifiers we will get output 1,2,3 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb39af7-3e43-4a28-a500-fd812e2c8ce8",
   "metadata": {},
   "source": [
    "<img src=\"images\\\\Ensemble_Techniques_D13.png\" alt=\"image\" width=\"500px\" border=\"2px\">\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9f18e-fbf7-499c-9883-bb2d436cd5ab",
   "metadata": {},
   "source": [
    "# Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8a5ba8ae-67e9-4424-b2be-1a173d5e7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "461d0f60-5856-465a-b14f-2cb45c14fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0       6   148    72    35     0  33.6  0.627   50      1\n",
       "1       1    85    66    29     0  26.6  0.351   31      0\n",
       "2       8   183    64     0     0  23.3  0.672   32      1\n",
       "3       1    89    66    23    94  28.1  0.167   21      0\n",
       "4       0   137    40    35   168  43.1  2.288   33      1\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
       "763    10   101    76    48   180  32.9  0.171   63      0\n",
       "764     2   122    70    27     0  36.8  0.340   27      0\n",
       "765     5   121    72    23   112  26.2  0.245   30      0\n",
       "766     1   126    60     0     0  30.1  0.349   47      1\n",
       "767     1    93    70    31     0  30.4  0.315   23      0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing file\n",
    "\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv(\"Datasets\\\\pima-indians-diabetes.data.csv\",names=names)   # dont have col names\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "982031e6-cf9c-4eaf-8dab-fe4fb278fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age\n",
       "0     6   148    72    35     0  33.6  0.627   50\n",
       "1     1    85    66    29     0  26.6  0.351   31\n",
       "2     8   183    64     0     0  23.3  0.672   32\n",
       "3     1    89    66    23    94  28.1  0.167   21\n",
       "4     0   137    40    35   168  43.1  2.288   33"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,0:8]\n",
    "y = df.iloc[:,8]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5d0c4fc6-84f5-4ca0-8634-5a5a9c7637ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72727273, 0.75324675, 0.7012987 , 0.85714286, 0.79220779,\n",
       "       0.71428571, 0.77922078, 0.76623377, 0.67105263, 0.80263158])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bagged Decision Trees for classification\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "kfold = KFold(n_splits=10,random_state=42,shuffle=True)\n",
    "# shuffle=True indicates that the data should be shuffled before splitting it into folds.\n",
    "# Shuffling ensures that the data is randomly distributed across the folds.\n",
    "\n",
    "\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "\n",
    "\n",
    "model = BaggingClassifier(estimator=cart,n_estimators=num_trees,random_state=42)\n",
    "results = cross_val_score(model,X,y,cv=kfold)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "11691799-64af-4997-a3b1-b89e65a0a6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564593301435407"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "998d1fe7-a387-45c0-bea1-258b7a08ef37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67532468, 0.76623377, 0.72727273, 0.83116883, 0.83116883,\n",
       "       0.71428571, 0.79220779, 0.72727273, 0.67105263, 0.80263158])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# using same k_fold\n",
    "\n",
    "num_trees = 100\n",
    "max_features = 3 #Total features m=8, K=3 i.e. randomly select 3 features\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=num_trees,max_features=max_features)   #n_estimators=10 by default, default=gini\n",
    "results = cross_val_score(model,X,y,cv=kfold)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5832834d-ab77-41ad-afe0-694568cb90b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753861927546138"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1dc9f8fa-53a7-431d-9534-93149ae79bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74025974, 0.80519481, 0.71428571, 0.79220779, 0.79220779,\n",
       "       0.67532468, 0.77922078, 0.79220779, 0.67105263, 0.81578947])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "num_trees = 10\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=num_trees,random_state=42)  #n_estimators=50 by default\n",
    "results = cross_val_score(model,X,y,cv=kfold)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4d3b8c01-a7bd-4dc8-9a17-a77010f40080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7577751196172249"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "46880bcf-10c1-4d10-b4d4-5315412ba0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75324675, 0.77922078, 0.71428571, 0.81818182, 0.81818182,\n",
       "       0.7012987 , 0.81818182, 0.76623377, 0.71052632, 0.81578947])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking Ensemble for Classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC   # support vector classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# create sub-models\n",
    "estimators=[]\n",
    "\n",
    "model1 = LogisticRegression(max_iter=500)\n",
    "estimators.append(('logistic', model1))\n",
    "\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "model3 = SVC()\n",
    "estimators.append(('svc', model3))\n",
    "\n",
    "# create ensemble model\n",
    "\n",
    "model = VotingClassifier(estimators)\n",
    "results = cross_val_score(model,X,y,cv=kfold)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "459029df-9879-4d84-9226-d7444f084c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic', LogisticRegression(max_iter=500)),\n",
       " ('cart', DecisionTreeClassifier()),\n",
       " ('svc', SVC())]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2b7c6f08-7776-4635-b693-e58409f420df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7695146958304853"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff301b7-e137-4b18-b1d8-e7bd85853638",
   "metadata": {},
   "source": [
    "# **Additional Code: \"random_state = any int\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f9b3074d-c6e3-4563-a031-8e7d45b15140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set X: [5 0 2 6 9 4 3]\n",
      "Testing set X: [7 1 8]\n",
      "Training labels y: [5 0 2 6 9 4 3]\n",
      "Testing labels y: [7 1 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFirst time Output:\\nTraining set X: [0 7 2 9 4 3 6]\\nTesting set X: [8 1 5]\\nTraining labels y: [0 7 2 9 4 3 6]\\nTesting labels y: [8 1 5]\\n\\nSecond time Output\\nSame as above\\n\\nTRY TO CHANGE random_state = ANY OTHER INTEGER\\n'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Generate data for the example (values from 0 to 10)\n",
    "data = np.arange(10)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,data, test_size=0.3, random_state=20)\n",
    "\n",
    "# Display the training and testing sets\n",
    "print(\"Training set X:\", X_train)\n",
    "print(\"Testing set X:\", X_test)\n",
    "print(\"Training labels y:\", y_train)\n",
    "print(\"Testing labels y:\", y_test)\n",
    "\n",
    "'''\n",
    "First time Output:\n",
    "Training set X: [0 7 2 9 4 3 6]\n",
    "Testing set X: [8 1 5]\n",
    "Training labels y: [0 7 2 9 4 3 6]\n",
    "Testing labels y: [8 1 5]\n",
    "\n",
    "Second time Output\n",
    "Same as above\n",
    "\n",
    "TRY TO CHANGE random_state = ANY OTHER INTEGER\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d8367-c308-4f80-b43c-18acf5f1b65c",
   "metadata": {},
   "source": [
    "# **If random_state is not passed as an argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "25900ac7-888c-4a1c-84b6-b6325138d2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set X: [7 1 4 3 5 8 2]\n",
      "Testing set X: [0 6 9]\n",
      "Training labels y: [7 1 4 3 5 8 2]\n",
      "Testing labels y: [0 6 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFirst time Output\\nTraining set X: [4 3 7 9 1 6 8]\\nTesting set X: [0 2 5]\\nTraining labels y: [4 3 7 9 1 6 8]\\nTesting labels y: [0 2 5]\\n\\nSecond time Output\\nTraining set X: [6 8 2 3 7 1 9]\\nTesting set X: [5 0 4]\\nTraining labels y: [6 8 2 3 7 1 9]\\nTesting labels y: [5 0 4]\\n'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Generate data for the example (values from 0 to 99)\n",
    "data = np.arange(10)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data, test_size=0.3)\n",
    "\n",
    "# Display the training and testing sets\n",
    "print(\"Training set X:\", X_train)\n",
    "print(\"Testing set X:\", X_test)\n",
    "print(\"Training labels y:\", y_train)\n",
    "print(\"Testing labels y:\", y_test)\n",
    "\n",
    "'''\n",
    "First time Output\n",
    "Training set X: [4 3 7 9 1 6 8]\n",
    "Testing set X: [0 2 5]\n",
    "Training labels y: [4 3 7 9 1 6 8]\n",
    "Testing labels y: [0 2 5]\n",
    "\n",
    "Second time Output\n",
    "Training set X: [6 8 2 3 7 1 9]\n",
    "Testing set X: [5 0 4]\n",
    "Training labels y: [6 8 2 3 7 1 9]\n",
    "Testing labels y: [5 0 4]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c794f-62d4-43da-8ec1-d20151baea29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
